/*
 * Mistral AI API
 *
 * Our Chat Completion and Embeddings APIs specification. Create your account on [La Plateforme](https://console.mistral.ai) to get access and read the [docs](https://docs.mistral.ai) to learn how to use it.
 *
 * The version of the OpenAPI document: 0.0.2
 *
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct FimCompletionRequest {
    #[serde(rename = "model", deserialize_with = "Option::deserialize")]
    pub model: Option<String>,
    #[serde(
        rename = "temperature",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub temperature: Option<Option<f64>>,
    /// Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
    #[serde(rename = "top_p", skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f64>,
    #[serde(
        rename = "max_tokens",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub max_tokens: Option<Option<i32>>,
    /// Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: \[DONE\] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.
    #[serde(rename = "stream", skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,
    #[serde(rename = "stop", skip_serializing_if = "Option::is_none")]
    pub stop: Option<Box<models::Stop>>,
    #[serde(
        rename = "random_seed",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub random_seed: Option<Option<i32>>,
    /// The text/code to complete.
    #[serde(rename = "prompt")]
    pub prompt: String,
    #[serde(
        rename = "suffix",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub suffix: Option<Option<String>>,
    #[serde(
        rename = "min_tokens",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub min_tokens: Option<Option<i32>>,
}

impl FimCompletionRequest {
    pub fn new(model: Option<String>, prompt: String) -> FimCompletionRequest {
        FimCompletionRequest {
            model,
            temperature: None,
            top_p: None,
            max_tokens: None,
            stream: None,
            stop: None,
            random_seed: None,
            prompt,
            suffix: None,
            min_tokens: None,
        }
    }
}
